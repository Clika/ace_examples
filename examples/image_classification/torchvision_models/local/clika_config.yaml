# ================= CLIKA ACE hyperparameter configuration file ================= #
# for more information please refer to https://docs.clika.io


deployment_settings:
  # Choose the target framework ["tflite", "ov", "ort", "trt", "qnn"]
  target_framework: trt

  # Set to True if you're planning to run the model on a CPU that supports AVX512-VNNI or if you're planning to run on ARM CPU.
  #   This setting is only relevant for "ov" (OpenVINO), "ort" (ONNXRuntime)
  # weights_utilize_full_int_range: false


distributed_training_settings:
  # enable multi gpu training
  multi_gpu: true

  # enable FSDP if true else use DDP
  use_sharding: false


global_quantization_settings:
  method: qat
  weights_num_bits: 8
  activations_num_bits: 8

  # whether to skip quantization for the tail of the model (keep it null if unsure)
  skip_tail_quantization: true

  # whether to automatically skip quantization for sensitive layers (keep it true if unsure)
  #      The threshold to decide automatically whether to skip quantization for layers that are too sensitive.
  #      This will only be applied if 'automatic_skip_quantization' is True.
  #      Some tips:
  #          * For small models like MobileNet - 5000 is a good value
  #          * For big models 10000 is a good value
  #      The quantization sensitivity is measured using L2(QuantizedTensor-FloatTensor), the higher it is the more "destructive" the quantization is.
  #      This also implies that it can take longer for a Model to recover it's performance if it is overly sensitive.
  automatic_skip_quantization: true
  quantization_sensitivity_threshold: null


training_settings:
  # Number of steps for initial calibration
  stats_steps: 20

  # Random seed applied on CLIKA SDK
  random_seed: null

  # Indicates CLIKA SDK that the model has untrained weight
  is_training_from_scratch: false


# Layer compression setting
#layer_settings:
#   "conv":
#    quantization_settings:
#      weights_num_bits: 8
#      activations_num_bits: 8


# Uncomment to apply LoRA
#global_lora_settings:
#   rank: 2
#   alpha: 1
#   dropout_rate: 0.05
